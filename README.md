# Self-attention-from-scratch
A project that builds a self attention block from scratch.

I intend to frame this project using the following steps:

1. Creating a basic word embedding of every word of the input sequence.

2. Creating Key, Value and Query matrix weights of each embedding.

3. Creating contextual embeddings of each word in the sentence using the Query, Key and Value matrices.

4. Usage of the contextual embeddings for generation/classification tasks.

5. Updating the weights according to their performance.

# This repository will be used for experimentation purposes. If you have any idea/ suggestion that you want to try out at any step, please let me know. I will try that and check the results. I am open to feedback and new suggestions at every step.
